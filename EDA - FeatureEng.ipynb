{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA - Feature Engineering\n",
    "\n",
    "### In the following document, this will outline some concepts to be used in performing Exploratory Data Analysis and prep data for machine learning.  This is not exhaustive and will cover the following:\n",
    "\n",
    "1. Missing values\n",
    "2. Temporal variables\n",
    "3. Non-Gaussian distributed variables\n",
    "4. Categorical variables: remove rare labels\n",
    "5. Categorical variables: convert strings to numbers\n",
    "6. Standarise the values of the variables to the same range\n",
    "\n",
    "### Setting the seed\n",
    "It is important to note that we are engineering variables and pre-processing data with the idea of deploying the model if we find business value in it. Therefore, for each step that includes some element of randomness, it is extremely important that we set the seed. This way, we can obtain reproducibility between our research and our development code.\n",
    "\n",
    "### Code vs Pseudocode\n",
    "\n",
    "Most of this is actual code.  However, since there is no dataset included, we will use the variable **data** to represent data that has been loaded in.\n",
    "\n",
    "Other conventions, where needed will use the format **target_col** for the column that we are looking to predict or classify, **num_col** for numerical column, **cat_col** for categorical column, **str_col** for string column, **date_col** for columns containing dates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some standard imports\n",
    "# to handle datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# to divide train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# feature scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# to visualise al the columns in the dataframe\n",
    "pd.pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data from a csv\n",
    "\n",
    "data = pd.read_csv('file.csv')\n",
    "\n",
    "# get the dimensions\n",
    "print(data.shape)\n",
    "\n",
    "# get a look\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's separate into train and test set\n",
    "# this can also be done at a later step\n",
    "# Remember to set the seed (random_state for this sklearn function)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data.target_col,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=0) # we are setting the seed here\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "For categorical variables, fill missing information by adding an additional category: \"missing\".  This allows for easier manipulation and understanding of how missing impacts the data better than **NaN**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a list of the categorical variables that contain missing values\n",
    "vars_with_na = [var for var in data.columns if X_train[var].isnull().sum()>1 and X_train[var].dtypes=='O']\n",
    "\n",
    "# print the variable name and the percentage of missing values\n",
    "for var in vars_with_na:\n",
    "    print(var, np.round(X_train[var].isnull().mean(), 3),  ' % missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to replace NA in categorical variables with missing\n",
    "def fill_categorical_na(df, var_list):\n",
    "    X = df.copy()\n",
    "    X[var_list] = df[var_list].fillna('Missing')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace missing values with new label: \"Missing\"\n",
    "X_train = fill_categorical_na(X_train, vars_with_na)\n",
    "X_test = fill_categorical_na(X_test, vars_with_na)\n",
    "\n",
    "# check that we have no missing information in the engineered variables\n",
    "# they should have now been replaced with 'missing'\n",
    "X_train[vars_with_na].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check that test set does not contain null values in the engineered variables\n",
    "[vr for var in vars_with_na if X_test[var].isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical variables, add an additional variable capturing the missing information, and then replace the missing information in the original variable by the mode, or most frequent value.  Sometimes, it's more common to use the mean, but that can capture outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a list of the numerical variables that contain missing values\n",
    "# note the dtypes!='O' for numerical\n",
    "\n",
    "vars_with_na = [var for var in data.columns if X_train[var].isnull().sum()>1 and X_train[var].dtypes!='O']\n",
    "\n",
    "# print the variable name and the percentage of missing values\n",
    "for var in vars_with_na:\n",
    "    print(var, np.round(X_train[var].isnull().mean(), 3),  ' % missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that test set does not contain null values in the engineered variables\n",
    "[vr for var in vars_with_na if X_test[var].isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal variables\n",
    "\n",
    "If there are variables that refer to the years in which something was something specific happened. Capture the time elapsed between the that variable and the event:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore the relationship between the year variables\n",
    "\n",
    "def elapsed_years(df, var):\n",
    "    # capture difference between year variable and year of event\n",
    "    df[var] = df['year_col'] - df[var]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in ['date_col1', 'date_col2', 'date_col3']:\n",
    "    X_train = elapsed_years(X_train, var)\n",
    "    X_test = elapsed_years(X_test, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that test set does not contain null values in the engineered variables\n",
    "[vr for var in ['date_col1', 'date_col2', 'date_col3'] if X_test[var].isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical variables\n",
    "\n",
    "Use the log transform the numerical variables that do not contain zeros in order to get a more Gaussian-like distribution. This tends to help Linear machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in ['num_col1', 'num_col2', 'num_col3']:\n",
    "    X_train[var] = np.log(X_train[var])\n",
    "    X_test[var]= np.log(X_test[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that test and train set does not contain null values in the engineered variables\n",
    "\n",
    "[var for var in ['num_col1', 'num_col2', 'num_col3'] if X_test[var].isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables\n",
    "\n",
    "First, remove those categories within variables that are present in less than 1% of the observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture the categorical variables first\n",
    "\n",
    "cat_vars = [var for var in X_train.columns if X_train[var].dtype == 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # finds the labels that are shared by more than a certain % of the rows in the dataset\n",
    "def find_frequent_labels(df, var, rare_perc):\n",
    "    df = df.copy()\n",
    "    tmp = df.groupby(var)['target_col'].count() / len(df)\n",
    "    return tmp[tmp>rare_perc].index\n",
    "\n",
    "for var in cat_vars:\n",
    "    frequent_ls = find_frequent_labels(X_train, var, 0.01)\n",
    "    X_train[var] = np.where(X_train[var].isin(frequent_ls), X_train[var], 'Rare')\n",
    "    X_test[var] = np.where(X_test[var].isin(frequent_ls), X_test[var], 'Rare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we need to transform the strings of these variables into numbers.\n",
    "# We will do it so that we capture the relationship between the label and the target\n",
    "# this function will assign discrete values to the strings of the variables, \n",
    "# so that the smaller value corresponds to the smaller mean of target\n",
    "\n",
    "def replace_categories(train, test, var, target):\n",
    "    ordered_labels = train.groupby([var])[target].mean().sort_values().index\n",
    "    ordinal_label = {k:i for i, k in enumerate(ordered_labels, 0)} \n",
    "    train[var] = train[var].map(ordinal_label)\n",
    "    test[var] = test[var].map(ordinal_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for var in cat_vars:\n",
    "    replace_categories(X_train, X_test, var, 'target_col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the relationship between labels and target\n",
    "# this makes as series of bar plots between the variables and the target_col\n",
    "# each plot shows the variable components/values in relationship to the target_col\n",
    "# remember that the target is log-transformed, so differences may seem small.\n",
    "\n",
    "def analyse_vars(df, var):\n",
    "    df = df.copy()\n",
    "    df.groupby(var)['target_col'].median().plot.bar()\n",
    "    plt.title(var)\n",
    "    plt.ylabel('target_col')\n",
    "    plt.show()\n",
    "    \n",
    "for var in cat_vars:\n",
    "    analyse_vars(X_train, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "For use in linear models, features need to be either scaled or normalised. Not all models require it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas adds the Id column as the index when you load the dataset\n",
    "\n",
    "train_vars = [var for var in X_train.columns if var not in ['Id', 'target_col']]\n",
    "len(train_vars)\n",
    "\n",
    "# fit scaler\n",
    "scaler = MinMaxScaler() # create an instance\n",
    "scaler.fit(X_train[train_vars]) #  fit  the scaler to the train set for later use\n",
    "\n",
    "# transform the train and test set, and add on the Id and SalePrice variables\n",
    "X_train = pd.concat([X_train[['Id', 'target_col']].reset_index(drop=True),\n",
    "                    pd.DataFrame(scaler.transform(X_train[train_vars]), columns=train_vars)],\n",
    "                    axis=1)\n",
    "\n",
    "X_test = pd.concat([X_test[['Id', 'target_col']].reset_index(drop=True),\n",
    "                    pd.DataFrame(scaler.transform(X_test[train_vars]), columns=train_vars)],\n",
    "                    axis=1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
